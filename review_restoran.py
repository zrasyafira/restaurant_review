# -*- coding: utf-8 -*-
"""Review restoran

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h59GFGcHBF4nAMJGrViP-fDV0n68hIR-

# Import libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

# %matplotlib inline

import re
import string
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
nltk.download('wordnet')
import nltk
nltk.download('averaged_perceptron_tagger_eng')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from joblib import dump, load
import pickle

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score

"""# Data Understanding"""

!pip install -q kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! pip install --upgrade kaggle
! kaggle datasets download -d joebeachcapital/restaurant-reviews

! unzip restaurant-reviews.zip

data = pd.read_csv('Restaurant reviews.csv')

data.head()

data.info()

data = data.dropna(subset=['Review', 'Rating'])

duplicate = data[data.duplicated(keep=False)]
print("\nData yang duplikat:")
print(duplicate[['Reviewer', 'Review']])

"""semua review berasal dari orang yang berbeda"""

data = data.drop(columns=['Restaurant', 'Reviewer', 'Metadata', 'Time', 'Pictures', '7514'])

data['Rating'] = pd.to_numeric(data['Rating'], errors='coerce')

data.isna().sum()

data = data.dropna(subset=['Rating'])

data.info()

data['Review'].duplicated().sum()

duplicate = data[data.duplicated(keep=False)]
print("\nData yang duplikat:")
print(duplicate)

data.drop_duplicates(subset=['Review'], keep='first')

data['Rating'] = data['Rating'].astype(float).astype(int)

height = data['Rating'].value_counts()
labels = data['Rating'].unique()
y_pos = np.arange(len(labels))

plt.figure(figsize=(8,5), dpi=80)
plt.ylim(0,5500)
plt.title('Rating', fontweight='bold')
plt.xlabel('Rating', fontweight='bold')
plt.ylabel('Count', fontweight='bold')
plt.bar(y_pos, height, align='center', alpha=0.5)
plt.xticks(y_pos, labels)
plt.show()

"""# Case Folding"""

def contains_emoticon(text):
    emoticon_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F700-\U0001F77F"  # alchemical symbols
                               u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                               u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                               u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
                               u"\U0001FA00-\U0001FA6F"  # Chess Symbols
                               u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
                               u"\U00002702-\U000027B0"  # Dingbats
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return bool(emoticon_pattern.search(text))

data['contains_emoticon'] = data['Review'].apply(contains_emoticon)
print(data['contains_emoticon'].value_counts())

def remove_emoticon(text):
    emoticon_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F700-\U0001F77F"  # alchemical symbols
                               u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
                               u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
                               u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
                               u"\U0001FA00-\U0001FA6F"  # Chess Symbols
                               u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
                               u"\U00002702-\U000027B0"  # Dingbats
                               u"\U000024C2-\U0001F251"
                               "]+", flags=re.UNICODE)
    return emoticon_pattern.sub(r'', text)


cleaned_review = data['Review'].apply(remove_emoticon)
print(cleaned_review)

data['cleaned_review'] = data['Review'].apply(remove_emoticon)

def casefolding(text):
  text = text.lower()                               # Mengubah teks menjadi lower case
  text = re.sub(r'https?://\S+|www\.\S+', '', text) # Menghapus URL
  text = text.strip()
  return text

raw_review = data['cleaned_review'].iloc[5]
case_folding = casefolding(raw_review)

print('Raw data\t: ', raw_review)
print('Case folding\t: ', case_folding)

"""# Word Normalization & Lemmatization


"""

normalization_dict = {
    "n't": "not",
    "'re": "are",
    "'ve": "have",
    "'ll": "will",
    "'d": "would",
    "'m": "am"
}

def word_normalization(text):
    words = text.split()
    normalized_words = [normalization_dict[word] if word in normalization_dict else word for word in words]
    return ' '.join(normalized_words)

data['normalized_review'] = data['cleaned_review'].apply(word_normalization)

def get_wordnet_pos(word):
    from nltk.corpus import wordnet
    from nltk import pos_tag
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = text.split()
    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]
    return ' '.join(lemmatized_words)

data['lemmatized_review'] = data['normalized_review'].apply(lemmatize_text)

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

data['filtered_review'] = data['lemmatized_review'].apply(remove_stopwords)

raw_review = data['cleaned_review'].iloc[5]
case_folding = casefolding(raw_review)
normalization = word_normalization(case_folding)
lemmatization = lemmatize_text(normalization)
stopwords = remove_stopwords(lemmatization)

print('Raw data\t: ', raw_review)
print('Case folding\t: ', case_folding)
print('Normalization\t: ', normalization)
print('Lemmatization\t: ', lemmatization)
print('Stopwords\t: ', stopwords)

"""# Pre-Processing"""

def text_preprocessing(text):
    text = remove_emoticon(text)
    text = casefolding(text)
    text = word_normalization(text)
    text = lemmatize_text(text)
    text = remove_stopwords(text)
    return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data['preprocessed_review'] = data['Review'].apply(text_preprocessing)

data.head()

data.to_csv('preprocessed_data.csv', index=False)

X = data['preprocessed_review']
y = data['Rating']

X

y

"""# Feature Extraction (Bag of Words & N-gram)"""

# BoW - Unigram
bow = CountVectorizer(ngram_range=(1,1))
bow.fit(X)

print(len(bow.get_feature_names_out()))

print(bow.get_feature_names_out())

X_bow = bow.transform(X).toarray()
X_bow

with open('bow_feature.pickle', 'wb') as output:
  pickle.dump(X_bow, output)

"""# Feature Extraction (TF-IDF & N-gram)"""

tf_idf = TfidfVectorizer(ngram_range=(1,1))
tf_idf.fit(X)

print(len(tf_idf.get_feature_names_out()))

print(tf_idf.get_feature_names_out())

X_tf_idf = tf_idf.transform(X).toarray()
X_tf_idf

data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())
data_tf_idf

with open('tf_idf_feature.pickle', 'wb') as output:
  pickle.dump(X_tf_idf, output)

"""# Feature Extraction"""

X = np.array(data_tf_idf)
y = np.array(y)

chi2_features = SelectKBest(chi2, k=1000)
X_kbest_features = chi2_features.fit_transform(X, y)

print('Original feature number:', X.shape[1])
print('Reduced feature number:', X_kbest_features.shape[1])

data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])
data_chi2

feature = tf_idf.get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

data_chi2.sort_values(by='nilai', ascending=False)

mask = chi2_features.get_support()
mask

new_feature = []

for bool, f in zip(mask, feature):
  if bool:
    new_feature.append(f)
  selected_feature = new_feature

selected_feature

tf_idf.vocabulary_

kbest_feature = {} # Buat dictionary kosong

for (k,v) in tf_idf.vocabulary_.items():    # Iterasi untuk mengulangi vocab yang dihasilkan TF_IDF
  if k in selected_feature:                 # Cek apakah fitur termasuk k fitur yang diseleksi
    kbest_feature[k] = v                    # Jika iya, simpan fitur tersebut pada dictionary kosong diatas

kbest_feature

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

with open('kbest_feature.pickle', 'wb') as output:
  pickle.dump(kbest_feature, output)

"""# Modelling"""

X_train, X_test, y_train, y_test = train_test_split(X_kbest_features, y, test_size=0.2, random_state=40)

"""naive bayes"""

algorithm = MultinomialNB()               # Load algoritma pembelajaran tertentu
model_nb = algorithm.fit(X_train, y_train)   # Fitkan (latih) algoritman menggunakan data latih & label latih

# Simpan model hasil traning
dump(model_nb, filename='model_1.joblib')

model_nb_pred = model_nb.predict(X_test)

model_nb_pred

"""logistic regression"""

algorithm = LogisticRegression(C= 1)
model_lr = algorithm.fit(X_train, y_train)

dump(model_lr, filename='model_2.joblib')

model_lr_pred = model_lr.predict(X_test)

model_lr_pred

"""knn"""

algorithm = KNeighborsClassifier(n_neighbors=5)
model_knn = algorithm.fit(X_train, y_train)

dump(model_knn, filename='model_3.joblib')

model_knn_pred = model_knn.predict(X_test)

model_knn_pred

"""# Model Evaluation

naive bayes
"""

prediksi_benar = (model_nb_pred == y_test).sum()
prediksi_salah = (model_nb_pred != y_test).sum()

print('Jumlah prediksi benar\t:', prediksi_benar)
print('Jumlah prediksi salah\t:', prediksi_salah)

accuracy = prediksi_benar / (prediksi_benar + prediksi_salah)*100
print('Accuracy pengujian\t:', accuracy, '%')

cm = confusion_matrix(y_test, model_nb_pred)
print('Confusion matrix:\n', cm)

print('Classification report:\n', classification_report(y_test, model_nb_pred))

"""logistic regression"""

prediksi_benar = (model_lr_pred == y_test).sum()
prediksi_salah = (model_lr_pred != y_test).sum()

print('Jumlah prediksi benar\t:', prediksi_benar)
print('Jumlah prediksi salah\t:', prediksi_salah)

accuracy = prediksi_benar / (prediksi_benar + prediksi_salah)*100
print('Accuracy pengujian\t:', accuracy, '%')

cm = confusion_matrix(y_test, model_lr_pred)
print('Confusion matrix:\n', cm)

print('Classification report:\n', classification_report(y_test, model_lr_pred))

"""knn"""

prediksi_benar = (model_knn_pred == y_test).sum()
prediksi_salah = (model_knn_pred != y_test).sum()

print('Jumlah prediksi benar\t:', prediksi_benar)
print('Jumlah prediksi salah\t:', prediksi_salah)

accuracy = prediksi_benar / (prediksi_benar + prediksi_salah)*100
print('Accuracy pengujian\t:', accuracy, '%')

cm = confusion_matrix(y_test, model_knn_pred)
print('Confusion matrix:\n', cm)

print('Classification report:\n', classification_report(y_test, model_knn_pred))

models = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(C=1, max_iter=1000, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)
}

print("Cross-Validation Results:\n" + "-"*30)
for model_name, model in models.items():
    try:
        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        print(f"{model_name}:\n - Cross-validation scores: {scores}")
        print(f" - Average accuracy: {scores.mean():.4f}")
        print(f" - Standard deviation: {scores.std():.4f}\n")
    except Exception as e:
        print(f"Error with {model_name}: {e}\n")

"""# Deployment"""

pipelines = {
    "Naive Bayes": load('model_1.joblib'),
    "Logistic Regression": load('model_2.joblib'),
    "K-Nearest Neighbors": load('model_3.joblib')
}

vocab = load('kbest_feature.pickle')

category_mapping = {
    1: 'very bad',
    2: 'bad',
    3: 'neutral',
    4: 'good',
    5: 'excellent'
}

input_text = "the food isn't good but not bad either" #@param {type:"string"}

pre_input_text = text_preprocessing(input_text)   # lakukan text pre processing pada text input

def predict_with_models(input_text):

    tf_idf_vec = TfidfVectorizer(vocabulary=set(vocab))
    input_vector = tf_idf_vec.fit_transform([input_text])

    predictions = {}
    for model_name, model in pipelines.items():
        result = model.predict(input_vector)[0]
        predictions[model_name] = result

    return predictions

predictions = predict_with_models(pre_input_text)

print(f"\nHasil Text Preprocessing: {pre_input_text}") # Print the preprocessed text
print(f"Prediksi untuk teks: '{input_text}'")
for model_name, prediction in predictions.items():
    print(f"  - {model_name}: {prediction}")
    print(f"  - {model_name}: {category_mapping[prediction]}")